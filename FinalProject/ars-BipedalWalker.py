import gymnasium as gym
import numpy as np
from huggingface_hub import HfApi, upload_folder, login
from huggingface_hub.repocard import metadata_eval_result, metadata_save
from gym.wrappers import Monitor
from torch.utils.tensorboard import SummaryWriter
from stable_baselines3.common.env_util import make_vec_env
from pathlib import Path
import torch
import datetime
import tempfile
import json
import shutil
import imageio

def make_env(env_id, seed, idx, capture_video, run_name):
    def thunk():
        env = gym.make(env_id, hardcore=True)
        env = gym.wrappers.RecordEpisodeStatistics(env)
        if capture_video:
            if idx == 0:
                env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
        env.seed(seed)
        env.action_space.seed(seed)
        env.observation_space.seed(seed)
        return env

    return thunk

def _add_logdir(local_path: Path, logdir: Path):
  """Adds a logdir to the repository.
  :param local_path: repository directory
  :param logdir: logdir directory
  """
  if logdir.exists() and logdir.is_dir():
    # Add the logdir to the repository under new dir called logs
    repo_logdir = local_path / "logs"

    # Delete current logs if they exist
    if repo_logdir.exists():
      shutil.rmtree(repo_logdir)

    # Copy logdir into repo logdir
    shutil.copytree(logdir, repo_logdir)

def _save_model_card(local_path, generated_model_card, metadata):
    """Saves a model card for the repository.
    :param local_path: repository directory
    :param generated_model_card: model card generated by _generate_model_card()
    :param metadata: metadata
    """
    readme_path = local_path / "README.md"
    readme = ""
    if readme_path.exists():
        with readme_path.open("r", encoding="utf8") as f:
            readme = f.read()
    else:
        readme = generated_model_card

    with readme_path.open("w", encoding="utf-8") as f:
        f.write(readme)

    # Save our metrics to Readme metadata
    metadata_save(readme_path, metadata)

def generate_metadata(model_name, env_id, mean_reward, std_reward):
  """
  Define the tags for the model card
  :param model_name: name of the model
  :param env_id: name of the environment
  :mean_reward: mean reward of the agent
  :std_reward: standard deviation of the mean reward of the agent
  """
  metadata = {}
  metadata["tags"] = [
        env_id,
        "ars",
        "deep-reinforcement-learning",
        "reinforcement-learning",
        "custom-implementation",
		"BipedalWalkerHardcore-v3"
  ]

  # Add metrics
  eval = metadata_eval_result(
      model_pretty_name=model_name,
      task_pretty_name="reinforcement-learning",
      task_id="reinforcement-learning",
      metrics_pretty_name="mean_reward",
      metrics_id="mean_reward",
      metrics_value=f"{mean_reward:.2f} +/- {std_reward:.2f}",
      dataset_pretty_name=env_id,
      dataset_id=env_id,
  )

  # Merges both dictionaries
  metadata = {**metadata, **eval}

  return metadata

def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):
  """
  Generate the model card for the Hub
  :param model_name: name of the model
  :env_id: name of the environment
  :mean_reward: mean reward of the agent
  :std_reward: standard deviation of the mean reward of the agent
  :hyperparameters: training arguments
  """
  # Select the tags
  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)

  # Transform the hyperparams namespace to string
  converted_dict = vars(hyperparameters)
  converted_str = str(converted_dict)
  converted_str = converted_str.split(", ")
  converted_str = '\n'.join(converted_str)

  # Generate the model card
  model_card = f"""
  # ARS Agent Playing {env_id}

  This is a trained model of a ARS agent playing {env_id}.

  # Hyperparameters
  ```python
  {converted_str}
  ```
  """
  return model_card, metadata

def record_video(env, agent, out_directory, fps=30):
  images = []
  done = False
  state = env.reset()[0]
  img = env.render()
  images.append(img)
  while not done:
    action = agent.policy(state, agent.weights)
    state, reward, done, truncated, info = env.step(action)
    img = env.render()
    images.append(img)
  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)

def package_to_hub(repo_id,
                model,
                hyperparameters,
                eval_env,
                video_fps=30,
                commit_message="Push agent to the Hub",
                token= None,
                logs=None
                ):
  """
  Evaluate, Generate a video and Upload a model to Hugging Face Hub.
  This method does the complete pipeline:
  - It evaluates the model
  - It generates the model card
  - It generates a replay video of the agent
  - It pushes everything to the hub
  :param repo_id: id of the model repository from the Hugging Face Hub
  :param model: trained model
  :param eval_env: environment used to evaluate the agent
  :param fps: number of fps for rendering the video
  :param commit_message: commit message
  :param logs: directory on local machine of tensorboard logs you'd like to upload
  """

  # Clone or create the repo
  repo_url = HfApi().create_repo(
        repo_id=repo_id,
        token=token,
        private=False,
        exist_ok=True,
    )

  with tempfile.TemporaryDirectory() as tmpdirname:
    tmpdirname = Path(tmpdirname)

    # Evaluate the model and build JSON
    mean_reward, std_reward = model.evaluate(eval_env)

    # First get datetime
    eval_datetime = datetime.datetime.now()
    eval_form_datetime = eval_datetime.isoformat()

    evaluate_data = {
        "env_id": hyperparameters.env_id,
        "mean_reward": mean_reward,
        "std_reward": std_reward,
        "n_evaluation_episodes": hyperparameters.test_iterations,
        "eval_datetime": eval_form_datetime,
    }

    # Write a JSON file
    with open(tmpdirname / "results.json", "w") as outfile:
      json.dump(evaluate_data, outfile)

    # Generate a video
    video_path =  tmpdirname / "replay.mp4"
    record_video(gym.make(hyperparameters.env_id, render_mode='rgb_array', hardcore=True), model, video_path, video_fps)

    # Generate the model card
    generated_model_card, metadata = _generate_model_card(hyperparameters.algorithm, hyperparameters.env_id, mean_reward, std_reward, hyperparameters)
    _save_model_card(tmpdirname, generated_model_card, metadata)

    # Add logs if needed
    if logs:
      _add_logdir(tmpdirname, Path(logs))

    print(f"Pushing repo {repo_id} to the Hugging Face Hub")

    repo_url = upload_folder(
            repo_id=repo_id,
            folder_path=tmpdirname,
            path_in_repo="",
            commit_message=commit_message,
            token=token,
        )

    print(f"Your model is pushed to the Hub. You can view your model here: {repo_url}")
  return repo_url

### Solving means gettting over 300 points in 100 consecutive trials
### Only for simple version, hardcore cannot be trained with ARS

class HP():
	def __init__(self):
		self.v = 0.03			# Noise in delta
		self.N = 16				# No of perturbations
		self.b = 8				# No of top performing directions (<= self.N)
		self.lr = 0.02			# Learning rate
		self.normalizer = True	# Use normalizer
		self.writer = SummaryWriter()
		self.env_id = 'BipedalWalker-v3'
		self.algorithm = 'ARS'

		self.iterations = 3_000 # No of training iteration
		self.episode_length = 5000
		self.test_iterations = 100

		self.writer.add_scalar("Delta noise", self.v)
		self.writer.add_scalar("Perturbations", self.N)
		self.writer.add_scalar("Top performing directions", self.b)
		self.writer.add_scalar("Learning rate", self.lr)
		self.writer.add_scalar("Normalize states", self.normalizer)
		self.writer.add_scalar("Iterations", self.iterations)

class Agent:
	def __init__(self, hp):
		self.hp = hp
		self.weights = None

	def policy(self, state, weights):
		state = self.hp.normalizer.normalize(state)
		# print('Weights: ', weights.shape) 			                # weights : (4, 24)
		# print('State: ', state.shape)  				                  # state : (1, 24)
		return np.matmul(weights, state.reshape(-1,1)).flatten()	# action : (4, 1)

	def test_env(self, env, weights, test_only=False):
		state = env.reset()[0]
		done = False
		total_reward = 0.0
		steps = 0

		while not done and steps<self.hp.episode_length:
			if not test_only: self.hp.normalizer.observe(state) # obs space contains inf so we compute only what we observe

			action = self.policy(state, weights)
			next_state, reward, done, truncated, info = env.step(action)

			# Avoid being stuck (next_state[2] == velocity on x axis)
			if abs(next_state[2]) < 0.001:
				reward = -100
				done = True

			total_reward += reward
			steps += 1
			state = next_state
		
		return float(total_reward), steps
	
	def evaluate(self, env, episodes=100):
		rewards = []
		for ep in range(episodes):
			state = env.reset()[0]
			done = False
			total_reward = 0.0
			steps = 0

			while not done and steps<5000: # 5000 is episode_length
				env.render()
				
				action = self.policy(state, self.weights)
				next_state, reward, done, truncated, info = env.step(action)

				total_reward+=reward
				steps+=1
				state = next_state
			
			print(f'Episode: {ep}/{episodes}, Score: {total_reward}, Steps: {steps}')
			rewards += [total_reward]

		return np.mean(rewards), np.std(rewards)

# Ensures that policies put equal weight on every component of state 
#		-> otherwise dramatic changes in small ranges will only barely affect computation
class Normalizer():
	def __init__(self, nb_inputs):
		self.n = np.zeros(nb_inputs)
		self.mean = np.zeros(nb_inputs)
		self.mean_diff = np.zeros(nb_inputs)
		self.var = np.zeros(nb_inputs)

	def observe(self, x):
		self.n += 1.
		last_mean = self.mean.copy()
		self.mean += (x - self.mean) / self.n
		self.mean_diff += (x - last_mean) * (x - self.mean)
		self.var = (self.mean_diff / self.n).clip(min=1e-2)

	def normalize(self, inputs):
		obs_mean = self.mean
		obs_std = np.sqrt(self.var)
		return (inputs - obs_mean) / obs_std

	def store(self):
		np.savetxt('NormalizerMean.txt', self.mean)
		np.savetxt('NormalizerVar.txt', self.var)

class ARS:
	def __init__(self):
		self.hp = HP()
		self.env = gym.make("BipedalWalker-v3", hardcore=True)
		self.agent = Agent(self.hp)
		self.best_score = -1000
		self.desired_score = 300
		self.size = [self.env.action_space.shape[0], self.env.observation_space.shape[0]] # (4, 24)
		self.weights = np.zeros(self.size)		# 𝜃 parameters: shape (output_size, input_size)
		if self.hp.normalizer: 
			self.hp.normalizer = Normalizer([1,self.size[1]]) # (1, 24) -> normalizing the observation space
		else: 
			self.hp.normalizer=None
		self.plot = ModelPlot(self.hp.iterations)

	# Sort rewards in descending order based on (r(𝜃+𝛎𝜹), r(𝜃-𝛎𝜹)) and use only top b rewards with their perturbations 𝜹
	def sort_directions(self, reward_p, reward_n):
		reward_max = [max(rp, rn) for rp, rn in zip(reward_p, reward_n)]
		idx = np.argsort(reward_max)[::-1]	# Sort rewards in descending order and get indices.
		return idx

	def update_weights(self, reward_p, reward_n, delta):
		idx = self.sort_directions(reward_p, reward_n)

		step = np.zeros(self.weights.shape)
		for i in range(self.hp.b):
			step += [reward_p[idx[i]] - reward_n[idx[i]]]*delta[idx[i]] # (reward_p, reward_n, delta) == rollouts
		
		sigmaR = np.std(np.array(reward_p)[idx][:self.hp.b] + np.array(reward_n)[idx][:self.hp.b])
		# divide by standard deviation of the collected rewards (otherwise the variations can be too big)
		self.weights += self.hp.lr / (self.hp.b*sigmaR) * step

	def sample_delta(self, size):
		return [np.random.randn(*size) for _ in range(self.hp.N)]

	def save_policy(self):
		np.savetxt('BipedalWalkerHardcoreModel.txt', self.weights)
		self.hp.normalizer.store()

	def train_epoch(self):
		delta = self.sample_delta(self.size)

		reward_p = [self.agent.test_env(self.env, self.weights + self.hp.v*x)[0] for x in delta]
		reward_n = [self.agent.test_env(self.env, self.weights - self.hp.v*x)[0] for x in delta]
		
		self.update_weights(reward_p, reward_n, delta)

	def train(self):
		print('Training started...')

		for counter in range(self.hp.iterations):
			self.train_epoch()

			test_reward, test_steps = self.agent.test_env(self.env, self.weights, test_only=True)

			self.plot.rewards += [test_reward]
			self.plot.env_steps += [test_steps]
			self.plot.rewards_means += [np.mean(self.plot.rewards[-self.hp.test_iterations:])]

			self.hp.writer.add_scalar('Reward', test_reward, counter)
			self.hp.writer.add_scalar('Episode steps', test_steps, counter)
			self.hp.writer.add_scalar('Average Reward', np.mean(self.plot.rewards[-self.hp.test_iterations:]), counter)

			if np.mean(self.plot.rewards[-self.hp.test_iterations:]) > self.desired_score and test_reward > self.best_score:
				self.best_score = test_reward
				self.save_policy()
			print(f'Iteration: {counter} -> Reward: {test_reward} || Average: {np.mean(self.plot.rewards[-self.hp.test_iterations:])}')
			
		self.hp.writer.close()
		print('Training ended!')
		self.plot.plot_convergence()
		self.agent.weights = self.weights

	def evaluate(self):
		print('Evaluating agent...')

		package_to_hub(
			repo_id = "MadFritz/ars-BipedalWalkerHardcore-v3",
			model = self.agent,
			hyperparameters = self.hp,
			eval_env = gym.make(self.hp.env_id, hardcore=True),
			logs= f"runs/ars-BipedalWalkerHardcore-v3",
			)

### Plotting convergence ###
class ModelPlot():
	def __init__(self, iterations):
		self.steps = [i for i in range(iterations)]
		self.rewards = []
		self.env_steps = []
		self.rewards_means = []

	def plot_convergence(self):
		import matplotlib.pyplot as plt
		plt.plot(self.steps, self.rewards_means)
		plt.title('Bipedal Walker Hardcore-> reinforcement learning with ARS')
		plt.xlabel('Steps')
		plt.ylabel('Reward')
		plt.show()
		plt.savefig('BipedalWalkerHardcoreConvergence.png')

if __name__ == '__main__':
	login(token="HUGGINGFACE_TOKEN")

	ars = ARS()
	ars.train()
	ars.evaluate()
