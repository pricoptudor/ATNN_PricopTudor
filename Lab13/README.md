## Lab 13
***
For self-study:
 - Multi-Headed Attention [implementation](https://nn.labml.ai/transformers/mha.html) from Attention Is All You Need ([paper](https://arxiv.org/abs/1706.03762), [video](https://www.youtube.com/watch?v=iDulhoQ2pro)). 
 - Transformer Encoder and Decoder Models [implementation](https://nn.labml.ai/transformers/models.html)
 - Fixed Positional Encodings [implementation](https://nn.labml.ai/transformers/positional_encoding.html)

Supplementary materials: 
 - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT): [paper](https://arxiv.org/abs/2010.11929), [code](https://nn.labml.ai/transformers/vit/index.html), [video](https://www.youtube.com/watch?v=TrdevFK_am4)
 - BERT: [paper](https://arxiv.org/abs/1810.04805) [code](https://nn.labml.ai/transformers/mlm/index.html), [video](https://www.youtube.com/watch?v=-9evrZnBorM)
 - GPT-2: [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), [code](https://nn.labml.ai/transformers/gpt/index.html), [video](https://www.youtube.com/watch?v=u1_qMdb0kYU)

Transformer and ViT implementations:
* https://github.com/lucidrains/vit-pytorch
* https://github.com/lucidrains/x-transformers

***
References:
- https://jalammar.github.io/illustrated-transformer/
- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/
